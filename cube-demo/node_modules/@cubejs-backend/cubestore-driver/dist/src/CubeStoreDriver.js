"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.CubeStoreDriver = void 0;
const stream_1 = require("stream");
const zlib_1 = require("zlib");
const fs_1 = require("fs");
const fs_extra_1 = require("fs-extra");
const tempy_1 = __importDefault(require("tempy"));
const csv_write_stream_1 = __importDefault(require("csv-write-stream"));
const base_driver_1 = require("@cubejs-backend/base-driver");
const shared_1 = require("@cubejs-backend/shared");
const sqlstring_1 = require("sqlstring");
const node_fetch_1 = __importDefault(require("node-fetch"));
const WebSocketConnection_1 = require("./WebSocketConnection");
const GenericTypeToCubeStore = {
    string: 'varchar(255)',
    text: 'varchar(255)',
    uuid: 'varchar(64)',
    // Cube Store uses an old version of sql parser which doesn't support timestamp with custom precision, but
    // athena driver (I believe old version) allowed to use it
    'timestamp(3)': 'timestamp',
    // TODO comes from JDBC. We might consider decimal96 here
    bigdecimal: 'decimal'
};
class CubeStoreDriver extends base_driver_1.BaseDriver {
    constructor(config) {
        super();
        this.config = {
            batchingRowSplitCount: (0, shared_1.getEnv)('batchingRowSplitCount'),
            ...config,
            // We use ip here instead of localhost, because Node.js 18 resolve localhost to IPV6 by default
            // https://github.com/node-fetch/node-fetch/issues/1624
            host: config?.host || (0, shared_1.getEnv)('cubeStoreHost') || '127.0.0.1',
            port: config?.port || (0, shared_1.getEnv)('cubeStorePort') || '3030',
            user: config?.user || (0, shared_1.getEnv)('cubeStoreUser'),
            password: config?.password || (0, shared_1.getEnv)('cubeStorePass'),
        };
        this.baseUrl = (this.config.url || `ws://${this.config.host}:${this.config.port}/`).replace(/\/ws$/, '/').replace(/\/$/, '');
        this.connection = new WebSocketConnection_1.WebSocketConnection(`${this.baseUrl}/ws`);
    }
    async testConnection() {
        await this.query('SELECT 1', []);
    }
    async query(query, values, options) {
        const { inlineTables, ...queryTracingObj } = options ?? {};
        const sql = (0, sqlstring_1.format)(query, values || []);
        return this.connection.query(sql, inlineTables ?? [], { ...queryTracingObj, instance: (0, shared_1.getEnv)('instanceId') });
    }
    async release() {
        return this.connection.close();
    }
    informationSchemaQuery() {
        return `
      SELECT columns.column_name as ${this.quoteIdentifier('column_name')},
             columns.table_name as ${this.quoteIdentifier('table_name')},
             columns.table_schema as ${this.quoteIdentifier('table_schema')},
             columns.data_type as ${this.quoteIdentifier('data_type')}
      FROM information_schema.columns as columns
      WHERE columns.table_schema NOT IN ('information_schema', 'system')`;
    }
    createTableSqlWithOptions(tableName, columns, options) {
        let sql = this.createTableSql(tableName, columns);
        const params = [];
        const withEntries = [];
        if (options.inputFormat) {
            withEntries.push(`input_format = '${options.inputFormat}'`);
        }
        if (options.delimiter) {
            withEntries.push(`delimiter = '${options.delimiter}'`);
        }
        if (options.buildRangeEnd) {
            withEntries.push(`build_range_end = '${options.buildRangeEnd}'`);
        }
        if (options.sealAt) {
            withEntries.push(`seal_at = '${options.sealAt}'`);
        }
        if (options.selectStatement) {
            withEntries.push(`select_statement = ${(0, sqlstring_1.escape)(options.selectStatement)}`);
        }
        if (options.sourceTable) {
            withEntries.push(`source_table = ${(0, sqlstring_1.escape)(`CREATE TABLE ${options.sourceTable.tableName} (${options.sourceTable.types.map(t => `${t.name} ${this.fromGenericType(t.type)}`).join(', ')})`)}`);
        }
        if (options.streamOffset) {
            withEntries.push(`stream_offset = '${options.streamOffset}'`);
        }
        if (withEntries.length > 0) {
            sql = `${sql} WITH (${withEntries.join(', ')})`;
        }
        if (options.uniqueKey) {
            sql = `${sql} UNIQUE KEY (${options.uniqueKey})`;
        }
        if (options.aggregations) {
            sql = `${sql} ${options.aggregations}`;
        }
        if (options.indexes) {
            sql = `${sql} ${options.indexes}`;
        }
        if (options.files) {
            sql = `${sql} LOCATION ${options.files.map(() => '?').join(', ')}`;
            params.push(...options.files);
        }
        return sql;
    }
    createTableWithOptions(tableName, columns, options, queryTracingObj) {
        const sql = this.createTableSqlWithOptions(tableName, columns, options);
        const params = [];
        if (options.files) {
            params.push(...options.files);
        }
        return this.query(sql, params, queryTracingObj).catch(e => {
            e.message = `Error during create table: ${sql}: ${e.message}`;
            throw e;
        });
    }
    async getTablesQuery(schemaName) {
        return this.query(`SELECT table_name, build_range_end FROM information_schema.tables WHERE table_schema = ${this.param(0)}`, [schemaName]);
    }
    async getPrefixTablesQuery(schemaName, tablePrefixes) {
        const prefixWhere = tablePrefixes.map(_ => 'table_name LIKE CONCAT(?, \'%\')').join(' OR ');
        return this.query(`SELECT table_name, build_range_end FROM information_schema.tables WHERE table_schema = ${this.param(0)} AND (${prefixWhere})`, [schemaName].concat(tablePrefixes));
    }
    async tableColumnTypes(table) {
        const [schema, name] = table.split('.');
        const columns = await this.query(`SELECT column_name as ${this.quoteIdentifier('column_name')},
             table_name as ${this.quoteIdentifier('table_name')},
             table_schema as ${this.quoteIdentifier('table_schema')},
             data_type as ${this.quoteIdentifier('data_type')}
      FROM information_schema.columns
      WHERE table_name = ${this.param(0)} AND table_schema = ${this.param(1)}`, [name, schema]);
        return columns.map(c => ({ name: c.column_name, type: this.toGenericType(c.data_type) }));
    }
    quoteIdentifier(identifier) {
        return `\`${identifier}\``;
    }
    fromGenericType(columnType) {
        return GenericTypeToCubeStore[columnType] || super.fromGenericType(columnType);
    }
    toColumnValue(value, genericType) {
        if (genericType === 'timestamp' && typeof value === 'string') {
            return value && value.replace('Z', '');
        }
        if (genericType === 'boolean' && typeof value === 'string') {
            if (value.toLowerCase() === 'true') {
                return true;
            }
            if (value.toLowerCase() === 'false') {
                return false;
            }
        }
        return super.toColumnValue(value, genericType);
    }
    async uploadTableWithIndexes(table, columns, tableData, indexesSql, uniqueKeyColumns, queryTracingObj, externalOptions) {
        const createTableIndexes = externalOptions?.createTableIndexes;
        const aggregationsColumns = externalOptions?.aggregationsColumns;
        const indexes = createTableIndexes && createTableIndexes.length ? createTableIndexes.map(this.createIndexString).join(' ') : '';
        let hasAggregatingIndexes = false;
        if (createTableIndexes && createTableIndexes.length) {
            hasAggregatingIndexes = createTableIndexes.some((index) => index.type === 'aggregate');
        }
        const aggregations = hasAggregatingIndexes && aggregationsColumns && aggregationsColumns.length ? ` AGGREGATIONS (${aggregationsColumns.join(', ')})` : '';
        if (tableData.rowStream) {
            await this.importStream(columns, tableData, table, indexes, aggregations, queryTracingObj);
        }
        else if (tableData.csvFile) {
            await this.importCsvFile(tableData, table, columns, indexes, aggregations, queryTracingObj);
        }
        else if (tableData.streamingSource) {
            await this.importStreamingSource(columns, tableData, table, indexes, uniqueKeyColumns, queryTracingObj, externalOptions?.sealAt);
        }
        else if (tableData.rows) {
            await this.importRows(table, columns, indexes, aggregations, tableData, queryTracingObj);
        }
        else {
            throw new Error(`Unsupported table data passed to ${this.constructor}`);
        }
    }
    createIndexString(index) {
        const prefix = {
            regular: '',
            aggregate: 'AGGREGATE '
        }[index.type] || '';
        return `${prefix}INDEX ${index.indexName} (${index.columns.join(',')})`;
    }
    async importRows(table, columns, indexesSql, aggregations, tableData, queryTracingObj) {
        if (!columns || columns.length === 0) {
            throw new Error('Unable to import (as rows) in Cube Store: empty columns. Most probably, introspection has failed.');
        }
        await this.createTableWithOptions(table, columns, { indexes: indexesSql, aggregations, buildRangeEnd: queryTracingObj?.buildRangeEnd }, queryTracingObj);
        try {
            const batchSize = 2000; // TODO make dynamic?
            for (let j = 0; j < Math.ceil(tableData.rows.length / batchSize); j++) {
                const currentBatchSize = Math.min(tableData.rows.length - j * batchSize, batchSize);
                const indexArray = Array.from({ length: currentBatchSize }, (v, i) => i);
                const valueParamPlaceholders = indexArray.map(i => `(${columns.map((c, paramIndex) => this.param(paramIndex + i * columns.length)).join(', ')})`).join(', ');
                const params = indexArray.map(i => columns
                    .map(c => this.toColumnValue(tableData.rows[i + j * batchSize][c.name], c.type)))
                    .reduce((a, b) => a.concat(b), []);
                await this.query(`INSERT INTO ${table}
        (${columns.map(c => this.quoteIdentifier(c.name)).join(', ')})
        VALUES ${valueParamPlaceholders}`, params, queryTracingObj);
            }
        }
        catch (e) {
            await this.dropTable(table);
            throw e;
        }
    }
    async importCsvFile(tableData, table, columns, indexes, aggregations, queryTracingObj) {
        if (!columns || columns.length === 0) {
            throw new Error('Unable to import (as csv) in Cube Store: empty columns. Most probably, introspection has failed.');
        }
        const files = Array.isArray(tableData.csvFile) ? tableData.csvFile : [tableData.csvFile];
        const options = {
            buildRangeEnd: queryTracingObj?.buildRangeEnd,
            indexes,
            aggregations
        };
        if (files.length > 0) {
            options.inputFormat = tableData.csvNoHeader ? 'csv_no_header' : 'csv';
            if (tableData.csvDelimiter) {
                options.delimiter = tableData.csvDelimiter;
            }
            options.files = files;
        }
        return this.createTableWithOptions(table, columns, options, queryTracingObj);
    }
    async importStream(columns, tableData, table, indexes, aggregations, queryTracingObj) {
        if (!columns || columns.length === 0) {
            throw new Error('Unable to import (as stream) in Cube Store: empty columns. Most probably, introspection has failed.');
        }
        const tempFiles = [];
        try {
            const pipelinePromises = [];
            const filePromises = [];
            let currentFileStream = null;
            const options = {
                buildRangeEnd: queryTracingObj?.buildRangeEnd,
                indexes,
                aggregations
            };
            const { baseUrl } = this;
            let fileCounter = 0;
            this.createTableSql(table, columns);
            // eslint-disable-next-line no-unused-vars
            const createTableSqlWithoutLocation = this.createTableSqlWithOptions(table, columns, options);
            const getFileStream = () => {
                if (!currentFileStream) {
                    const writer = (0, csv_write_stream_1.default)({ headers: columns.map(c => c.name) });
                    const tempFile = tempy_1.default.file();
                    tempFiles.push(tempFile);
                    const gzipStream = (0, zlib_1.createGzip)();
                    pipelinePromises.push(new Promise((resolve, reject) => {
                        (0, stream_1.pipeline)(writer, gzipStream, (0, fs_1.createWriteStream)(tempFile), (err) => {
                            if (err) {
                                reject(err);
                            }
                            const fileName = `${table}-${fileCounter++}.csv.gz`;
                            filePromises.push((0, node_fetch_1.default)(`${baseUrl.replace(/^ws/, 'http')}/upload-temp-file?name=${fileName}`, {
                                method: 'POST',
                                body: (0, fs_1.createReadStream)(tempFile),
                            }).then(async (res) => {
                                if (res.status !== 200) {
                                    const error = await res.json();
                                    throw new Error(`Error during upload of ${fileName} create table: ${createTableSqlWithoutLocation}: ${error.error}`);
                                }
                                return fileName;
                            }));
                            resolve(null);
                        });
                        currentFileStream = { stream: writer, tempFile };
                    }));
                }
                if (!currentFileStream) {
                    throw new Error('Stream init error');
                }
                return currentFileStream;
            };
            let rowCount = 0;
            const endStream = (chunk, encoding, callback) => {
                const { stream } = getFileStream();
                currentFileStream = null;
                rowCount = 0;
                if (chunk) {
                    stream.end(chunk, encoding, callback);
                }
                else {
                    stream.end(callback);
                }
            };
            const { batchingRowSplitCount } = this.config;
            const outputStream = new stream_1.Writable({
                write(chunk, encoding, callback) {
                    rowCount++;
                    if (rowCount >= batchingRowSplitCount) {
                        endStream(chunk, encoding, callback);
                    }
                    else {
                        getFileStream().stream.write(chunk, encoding, callback);
                    }
                },
                final(callback) {
                    endStream(null, null, callback);
                },
                objectMode: true
            });
            await new Promise((resolve, reject) => (0, stream_1.pipeline)(tableData.rowStream, outputStream, (err) => (err ? reject(err) : resolve(null))));
            await Promise.all(pipelinePromises);
            const files = await Promise.all(filePromises);
            if (files.length > 0) {
                options.files = files.map(fileName => `temp://${fileName}`);
            }
            return this.createTableWithOptions(table, columns, options, queryTracingObj);
        }
        finally {
            await Promise.all(tempFiles.map(tempFile => (0, fs_extra_1.unlink)(tempFile)));
        }
    }
    async importStreamingSource(columns, tableData, table, indexes, uniqueKeyColumns, queryTracingObj, sealAt) {
        if (!uniqueKeyColumns) {
            throw new Error('Older version of orchestrator is being used with newer version of Cube Store driver. Please upgrade cube.js.');
        }
        await this.query(`CREATE SOURCE OR UPDATE ${this.quoteIdentifier(tableData.streamingSource.name)} as ? VALUES (${Object.keys(tableData.streamingSource.credentials).map(k => `${k} = ?`)})`, [tableData.streamingSource.type]
            .concat(Object.keys(tableData.streamingSource.credentials).map(k => tableData.streamingSource.credentials[k])), queryTracingObj);
        let locations = [`stream://${tableData.streamingSource.name}/${tableData.streamingTable}`];
        if (tableData.partitions) {
            locations = [];
            for (let i = 0; i < tableData.partitions; i++) {
                locations.push(`stream://${tableData.streamingSource.name}/${tableData.streamingTable}/${i}`);
            }
        }
        const options = {
            buildRangeEnd: queryTracingObj?.buildRangeEnd,
            uniqueKey: uniqueKeyColumns.join(','),
            indexes,
            files: locations,
            selectStatement: tableData.selectStatement,
            sourceTable: tableData.sourceTable,
            streamOffset: tableData.streamOffset,
            sealAt
        };
        return this.createTableWithOptions(table, columns, options, queryTracingObj);
    }
    capabilities() {
        return {
            csvImport: true,
            streamImport: true,
        };
    }
}
exports.CubeStoreDriver = CubeStoreDriver;
//# sourceMappingURL=CubeStoreDriver.js.map
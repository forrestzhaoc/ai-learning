#!/usr/bin/env python3
"""
æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰è„šæœ¬
ç”¨äºåˆ†æç‰¹å¾ç›¸å…³æ€§ï¼Œè¾…åŠ©ç‰¹å¾å·¥ç¨‹

ç”¨æ³•:
    python src/eda.py
    python src/eda.py --output eda_report.txt
    python src/eda.py --visualize  # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""
from __future__ import annotations

import argparse
from pathlib import Path
import sys

import numpy as np
import pandas as pd

# å°è¯•å¯¼å…¥å¯è§†åŒ–åº“
try:
    import matplotlib
    matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_VISUALIZATION = True
except ImportError:
    HAS_VISUALIZATION = False
    print("è­¦å‘Š: matplotlib æˆ– seaborn æœªå®‰è£…ï¼Œå¯è§†åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    from sklearn.preprocessing import LabelEncoder
except ImportError:
    LabelEncoder = None

# æ·»åŠ é¡¹ç›®è·¯å¾„
ROOT_DIR = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT_DIR))

DATA_DIR = ROOT_DIR / "data"


def load_data():
    """åŠ è½½æ•°æ®"""
    train_path = DATA_DIR / "train.csv"
    if not train_path.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {train_path}")
        print("è¯·ç¡®ä¿ data/train.csv æ–‡ä»¶å­˜åœ¨")
        sys.exit(1)
    
    train_df = pd.read_csv(train_path)
    print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®: {train_df.shape[0]} è¡Œ Ã— {train_df.shape[1]} åˆ—")
    return train_df


def analyze_target_correlations(train_df, target_col='SalePrice', top_n=20):
    """
    åˆ†æç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        target_col: ç›®æ ‡å˜é‡åˆ—å
        top_n: æ˜¾ç¤ºå‰ N ä¸ªç‰¹å¾
    """
    print("\n" + "=" * 80)
    print("1. ç‰¹å¾ä¸ç›®æ ‡å˜é‡ (SalePrice) çš„ç›¸å…³æ€§åˆ†æ")
    print("=" * 80)
    
    # è·å–æ•°å€¼å‹ç‰¹å¾
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    
    if target_col not in numeric_cols:
        print(f"é”™è¯¯: ç›®æ ‡å˜é‡ '{target_col}' ä¸æ˜¯æ•°å€¼å‹")
        return None
    
    # ç§»é™¤ç›®æ ‡å˜é‡å’ŒID
    numeric_cols = [col for col in numeric_cols if col not in [target_col, 'Id']]
    
    # è®¡ç®—ç›¸å…³æ€§
    correlations = {}
    for col in numeric_cols:
        corr = train_df[col].corr(train_df[target_col])
        if not pd.isna(corr):
            correlations[col] = corr
    
    # æŒ‰ç»å¯¹å€¼æ’åº
    correlations_sorted = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'ç‰¹å¾åç§°':<30} {'ç›¸å…³ç³»æ•°':>12} {'ç›¸å…³æ€§å¼ºåº¦':<15}")
    print("-" * 60)
    
    for feature, corr in correlations_sorted[:top_n]:
        # åˆ¤æ–­ç›¸å…³æ€§å¼ºåº¦
        abs_corr = abs(corr)
        if abs_corr >= 0.7:
            strength = "å¼ºç›¸å…³ â­â­â­"
        elif abs_corr >= 0.5:
            strength = "ä¸­ç­‰ç›¸å…³ â­â­"
        elif abs_corr >= 0.3:
            strength = "å¼±ç›¸å…³ â­"
        else:
            strength = "å¾ˆå¼±"
        
        sign = "+" if corr > 0 else "-"
        print(f"{feature:<30} {corr:>12.4f} {strength:<15}")
    
    print(f"\nå…±åˆ†æäº† {len(correlations)} ä¸ªæ•°å€¼ç‰¹å¾")
    print(f"æ˜¾ç¤ºå‰ {min(top_n, len(correlations))} ä¸ªç‰¹å¾")
    
    return dict(correlations_sorted)


def analyze_feature_correlations(train_df, target_col='SalePrice', threshold=0.7):
    """
    åˆ†æç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆæ£€æµ‹å¤šé‡å…±çº¿æ€§ï¼‰
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        target_col: ç›®æ ‡å˜é‡åˆ—å
        threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼ˆé«˜äºæ­¤å€¼è®¤ä¸ºé«˜åº¦ç›¸å…³ï¼‰
    """
    print("\n" + "=" * 80)
    print("2. ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æï¼ˆæ£€æµ‹å¤šé‡å…±çº¿æ€§ï¼‰")
    print("=" * 80)
    
    # è·å–æ•°å€¼å‹ç‰¹å¾
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col not in [target_col, 'Id']]
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = train_df[numeric_cols].corr()
    
    # æ‰¾å‡ºé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) >= threshold:
                col1 = corr_matrix.columns[i]
                col2 = corr_matrix.columns[j]
                high_corr_pairs.append((col1, col2, corr_val))
    
    # æŒ‰ç›¸å…³æ€§ç»å¯¹å€¼æ’åº
    high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
    
    if high_corr_pairs:
        print(f"\nå‘ç° {len(high_corr_pairs)} å¯¹é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼ˆç›¸å…³ç³»æ•° >= {threshold}ï¼‰ï¼š")
        print(f"\n{'ç‰¹å¾1':<25} {'ç‰¹å¾2':<25} {'ç›¸å…³ç³»æ•°':>12}")
        print("-" * 65)
        for col1, col2, corr in high_corr_pairs[:20]:  # æ˜¾ç¤ºå‰20å¯¹
            print(f"{col1:<25} {col2:<25} {corr:>12.4f}")
        
        print(f"\nâš ï¸  å»ºè®®ï¼šé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯èƒ½å­˜åœ¨å¤šé‡å…±çº¿æ€§é—®é¢˜")
        print("   å¯ä»¥è€ƒè™‘ï¼š")
        print("   1. åˆ é™¤å…¶ä¸­ä¸€ä¸ªç‰¹å¾")
        print("   2. åˆ›å»ºç»„åˆç‰¹å¾")
        print("   3. ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰")
    else:
        print(f"\nâœ“ æœªå‘ç°é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹ï¼ˆé˜ˆå€¼: {threshold}ï¼‰")
    
    return high_corr_pairs


def analyze_missing_correlations(train_df, target_col='SalePrice'):
    """
    åˆ†æç¼ºå¤±å€¼ä¸ç›®æ ‡å˜é‡çš„å…³ç³»
    """
    print("\n" + "=" * 80)
    print("3. ç¼ºå¤±å€¼åˆ†æ")
    print("=" * 80)
    
    missing_data = train_df.isnull().sum()
    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
    
    if len(missing_data) > 0:
        print(f"\nå‘ç° {len(missing_data)} ä¸ªç‰¹å¾æœ‰ç¼ºå¤±å€¼ï¼š")
        print(f"\n{'ç‰¹å¾åç§°':<30} {'ç¼ºå¤±æ•°é‡':>12} {'ç¼ºå¤±æ¯”ä¾‹':>12}")
        print("-" * 55)
        
        for feature, count in missing_data.items():
            pct = (count / len(train_df)) * 100
            print(f"{feature:<30} {count:>12} {pct:>11.2f}%")
        
        # åˆ†æç¼ºå¤±å€¼æ˜¯å¦ä¸ç›®æ ‡å˜é‡ç›¸å…³
        print("\nåˆ†æç¼ºå¤±å€¼æ¨¡å¼ä¸ç›®æ ‡å˜é‡çš„å…³ç³»ï¼š")
        print("-" * 55)
        
        for feature in missing_data.head(10).index:
            if feature != target_col:
                # æ¯”è¾ƒæœ‰ç¼ºå¤±å€¼å’Œæ— ç¼ºå¤±å€¼çš„ä»·æ ¼å·®å¼‚
                missing_mask = train_df[feature].isnull()
                if missing_mask.sum() > 0:
                    price_with_missing = train_df.loc[missing_mask, target_col].mean()
                    price_without_missing = train_df.loc[~missing_mask, target_col].mean()
                    
                    if not pd.isna(price_with_missing) and not pd.isna(price_without_missing):
                        diff = price_with_missing - price_without_missing
                        print(f"{feature:<30} ç¼ºå¤±æ—¶å¹³å‡ä»·æ ¼: ${price_without_missing:,.0f}, "
                              f"æœ‰å€¼æ—¶: ${price_without_missing:,.0f}, "
                              f"å·®å¼‚: ${diff:,.0f}")
    else:
        print("\nâœ“ æœªå‘ç°ç¼ºå¤±å€¼")


def analyze_feature_importance_for_engineering(train_df, target_col='SalePrice'):
    """
    ä¸ºç‰¹å¾å·¥ç¨‹æä¾›å»ºè®®
    """
    print("\n" + "=" * 80)
    print("4. ç‰¹å¾å·¥ç¨‹å»ºè®®")
    print("=" * 80)
    
    # è·å–é«˜ç›¸å…³æ€§ç‰¹å¾
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col not in [target_col, 'Id']]
    
    correlations = {}
    for col in numeric_cols:
        corr = train_df[col].corr(train_df[target_col])
        if not pd.isna(corr):
            correlations[col] = abs(corr)
    
    # æ‰¾å‡ºé«˜ç›¸å…³æ€§ç‰¹å¾
    high_corr_features = [col for col, corr in correlations.items() if abs(corr) >= 0.5]
    
    print("\nåŸºäºç›¸å…³æ€§åˆ†æçš„ç‰¹å¾å·¥ç¨‹å»ºè®®ï¼š")
    print("-" * 80)
    
    # é¢ç§¯ç›¸å…³ç‰¹å¾
    area_features = [col for col in high_corr_features if 'SF' in col or 'Area' in col]
    if area_features:
        print(f"\nğŸ“ é¢ç§¯ç›¸å…³ç‰¹å¾ï¼ˆé«˜ç›¸å…³æ€§ï¼‰: {', '.join(area_features[:5])}")
        print("   å»ºè®®ï¼š")
        print("   - åˆ›å»ºæ€»é¢ç§¯ç‰¹å¾ï¼ˆTotalSF = åœ°ä¸‹å®¤ + ä¸€æ¥¼ + äºŒæ¥¼ï¼‰")
        print("   - åˆ›å»ºé¢ç§¯æ¯”ä¾‹ç‰¹å¾ï¼ˆå¦‚ï¼šåœ°ä¸‹å®¤å æ¯”ï¼‰")
        print("   - è€ƒè™‘é¢ç§¯çš„å¤šé¡¹å¼ç‰¹å¾ï¼ˆå¹³æ–¹ã€ç«‹æ–¹ï¼‰")
    
    # è´¨é‡ç›¸å…³ç‰¹å¾
    qual_features = [col for col in high_corr_features if 'Qual' in col or 'Cond' in col]
    if qual_features:
        print(f"\nâ­ è´¨é‡ç›¸å…³ç‰¹å¾ï¼ˆé«˜ç›¸å…³æ€§ï¼‰: {', '.join(qual_features[:5])}")
        print("   å»ºè®®ï¼š")
        print("   - åˆ›å»ºæ€»è´¨é‡è¯„åˆ†ï¼ˆTotalQual = å„è´¨é‡ç‰¹å¾ä¹‹å’Œï¼‰")
        print("   - å°†è´¨é‡æ–‡å­—è½¬æ¢ä¸ºæ•°å€¼ï¼ˆPo=1, Fa=2, TA=3, Gd=4, Ex=5ï¼‰")
    
    # æ—¶é—´ç›¸å…³ç‰¹å¾
    year_features = [col for col in high_corr_features if 'Year' in col or 'Yr' in col]
    if year_features:
        print(f"\nğŸ“… æ—¶é—´ç›¸å…³ç‰¹å¾ï¼ˆé«˜ç›¸å…³æ€§ï¼‰: {', '.join(year_features)}")
        print("   å»ºè®®ï¼š")
        print("   - åˆ›å»ºæˆ¿å±‹å¹´é¾„ç‰¹å¾ï¼ˆHouseAge = YrSold - YearBuiltï¼‰")
        print("   - åˆ›å»ºæ”¹å»ºå¹´é¾„ç‰¹å¾ï¼ˆRemodAge = YrSold - YearRemodAddï¼‰")
    
    # äºŒå…ƒç‰¹å¾å»ºè®®
    print(f"\nğŸ”˜ äºŒå…ƒç‰¹å¾å»ºè®®ï¼š")
    print("   - å¯¹äºæœ‰/æ— ç±»å‹çš„ç‰¹å¾ï¼Œåˆ›å»ºäºŒå…ƒç‰¹å¾ï¼ˆHasGarage, HasBasement ç­‰ï¼‰")
    print("   - è¿™äº›ç‰¹å¾å¯èƒ½æ•æ‰åˆ°éçº¿æ€§å…³ç³»")
    
    # äº¤äº’ç‰¹å¾å»ºè®®
    top_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)[:5]
    if len(top_features) >= 2:
        print(f"\nğŸ”— äº¤äº’ç‰¹å¾å»ºè®®ï¼š")
        print(f"   - è€ƒè™‘åˆ›å»ºäº¤äº’ç‰¹å¾ï¼ˆå¦‚ï¼š{top_features[0][0]} Ã— {top_features[1][0]}ï¼‰")
        print("   - äº¤äº’ç‰¹å¾å¯èƒ½æ•æ‰åˆ°ç‰¹å¾ä¹‹é—´çš„ååŒæ•ˆåº”")


def plot_correlation_heatmap(train_df, target_col='SalePrice', top_n=20, save_path=None):
    """
    ç»˜åˆ¶ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        target_col: ç›®æ ‡å˜é‡åˆ—å
        top_n: æ˜¾ç¤ºå‰ N ä¸ªç‰¹å¾
        save_path: ä¿å­˜è·¯å¾„
    """
    if not HAS_VISUALIZATION:
        print("è­¦å‘Š: å¯è§†åŒ–åº“æœªå®‰è£…ï¼Œè·³è¿‡çƒ­åŠ›å›¾ç”Ÿæˆ")
        return
    
    # è·å–æ•°å€¼å‹ç‰¹å¾
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col not in [target_col, 'Id']]
    
    # è®¡ç®—ç›¸å…³æ€§
    correlations = {}
    for col in numeric_cols:
        corr = train_df[col].corr(train_df[target_col])
        if not pd.isna(corr):
            correlations[col] = corr
    
    # æŒ‰ç»å¯¹å€¼æ’åºï¼Œå–å‰ top_n
    correlations_sorted = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
    top_features = [feat for feat, _ in correlations_sorted[:top_n]]
    
    # è®¡ç®—è¿™äº›ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = train_df[top_features + [target_col]].corr()
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(14, 12))
    sns.heatmap(
        corr_matrix,
        annot=True,
        fmt='.2f',
        cmap='coolwarm',
        center=0,
        square=True,
        linewidths=0.5,
        cbar_kws={"shrink": 0.8},
        vmin=-1,
        vmax=1
    )
    plt.title(f'Top {top_n} Features Correlation Heatmap with {target_col}', fontsize=16, pad=20)
    plt.tight_layout()
    
    if save_path is None:
        save_path = ROOT_DIR / "correlation_heatmap.png"
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {save_path}")


def plot_top_correlations(train_df, target_col='SalePrice', top_n=15, save_path=None):
    """
    ç»˜åˆ¶ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾æ¡å½¢å›¾
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        target_col: ç›®æ ‡å˜é‡åˆ—å
        top_n: æ˜¾ç¤ºå‰ N ä¸ªç‰¹å¾
        save_path: ä¿å­˜è·¯å¾„
    """
    # è·å–æ•°å€¼å‹ç‰¹å¾
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col not in [target_col, 'Id']]
    
    # è®¡ç®—ç›¸å…³æ€§
    correlations = {}
    for col in numeric_cols:
        corr = train_df[col].corr(train_df[target_col])
        if not pd.isna(corr):
            correlations[col] = corr
    
    # æŒ‰ç»å¯¹å€¼æ’åºï¼Œå–å‰ top_n
    correlations_sorted = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
    top_features = [feat for feat, _ in correlations_sorted[:top_n]]
    top_corrs = [corr for _, corr in correlations_sorted[:top_n]]
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    plt.figure(figsize=(12, 8))
    colors = ['red' if x < 0 else 'blue' for x in top_corrs]
    bars = plt.barh(range(len(top_features)), top_corrs, color=colors, alpha=0.7)
    plt.yticks(range(len(top_features)), top_features)
    plt.xlabel('Correlation Coefficient', fontsize=12)
    plt.title(f'Top {top_n} Features Correlated with {target_col}', fontsize=14, pad=20)
    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
    plt.grid(axis='x', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, corr) in enumerate(zip(bars, top_corrs)):
        plt.text(corr + (0.02 if corr >= 0 else -0.02), i, f'{corr:.3f}',
                va='center', fontsize=9)
    
    plt.tight_layout()
    
    if save_path is None:
        save_path = ROOT_DIR / "top_correlations.png"
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ç›¸å…³æ€§æ¡å½¢å›¾å·²ä¿å­˜åˆ°: {save_path}")


def plot_scatter_top_features(train_df, target_col='SalePrice', top_n=6, save_path=None):
    """
    ç»˜åˆ¶ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾çš„æ•£ç‚¹å›¾
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        target_col: ç›®æ ‡å˜é‡åˆ—å
        top_n: æ˜¾ç¤ºå‰ N ä¸ªç‰¹å¾
        save_path: ä¿å­˜è·¯å¾„
    """
    # è·å–æ•°å€¼å‹ç‰¹å¾
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col not in [target_col, 'Id']]
    
    # è®¡ç®—ç›¸å…³æ€§
    correlations = {}
    for col in numeric_cols:
        corr = train_df[col].corr(train_df[target_col])
        if not pd.isna(corr):
            correlations[col] = corr
    
    # æŒ‰ç»å¯¹å€¼æ’åºï¼Œå–å‰ top_n
    correlations_sorted = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
    top_features = [feat for feat, _ in correlations_sorted[:top_n]]
    
    # åˆ›å»ºå­å›¾
    n_cols = 3
    n_rows = (top_n + n_cols - 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
    axes = axes.flatten() if top_n > 1 else [axes]
    
    for idx, feature in enumerate(top_features):
        ax = axes[idx]
        corr = correlations[feature]
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        ax.scatter(train_df[feature], train_df[target_col], alpha=0.5, s=20)
        ax.set_xlabel(feature, fontsize=10)
        ax.set_ylabel(target_col, fontsize=10)
        ax.set_title(f'{feature}\nCorr: {corr:.3f}', fontsize=11)
        ax.grid(alpha=0.3)
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(top_n, len(axes)):
        axes[idx].set_visible(False)
    
    plt.suptitle(f'Scatter Plots: Top {top_n} Features vs {target_col}', fontsize=14, y=1.02)
    plt.tight_layout()
    
    if save_path is None:
        save_path = ROOT_DIR / "scatter_top_features.png"
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ æ•£ç‚¹å›¾å·²ä¿å­˜åˆ°: {save_path}")


def analyze_categorical_correlations(train_df, target_col='SalePrice', top_n=15):
    """
    åˆ†æåˆ†ç±»ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„å…³ç³»ï¼ˆé€šè¿‡ç¼–ç åè®¡ç®—ç›¸å…³æ€§ï¼‰
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        target_col: ç›®æ ‡å˜é‡åˆ—å
        top_n: æ˜¾ç¤ºå‰ N ä¸ªç‰¹å¾
    """
    print("\n" + "=" * 80)
    print("5. åˆ†ç±»ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„å…³ç³»åˆ†æ")
    print("=" * 80)
    
    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
    
    if not categorical_cols:
        print("\næœªå‘ç°åˆ†ç±»ç‰¹å¾")
        return
    
    correlations = {}
    
    for col in categorical_cols:
        # ä½¿ç”¨ LabelEncoder ç¼–ç 
        le = LabelEncoder()
        try:
            encoded = le.fit_transform(train_df[col].fillna('Missing'))
            corr = np.corrcoef(encoded, train_df[target_col])[0, 1]
            if not pd.isna(corr):
                correlations[col] = abs(corr)
        except:
            continue
    
    if correlations:
        correlations_sorted = sorted(correlations.items(), key=lambda x: x[1], reverse=True)
        
        print(f"\n{'ç‰¹å¾åç§°':<30} {'ç›¸å…³ç³»æ•°':>12} {'ç›¸å…³æ€§å¼ºåº¦':<15}")
        print("-" * 60)
        
        for feature, corr in correlations_sorted[:top_n]:
            abs_corr = abs(corr)
            if abs_corr >= 0.5:
                strength = "ä¸­ç­‰ç›¸å…³ â­â­"
            elif abs_corr >= 0.3:
                strength = "å¼±ç›¸å…³ â­"
            else:
                strength = "å¾ˆå¼±"
            
            print(f"{feature:<30} {corr:>12.4f} {strength:<15}")
        
        print(f"\nå…±åˆ†æäº† {len(correlations)} ä¸ªåˆ†ç±»ç‰¹å¾")
    else:
        print("\næ— æ³•è®¡ç®—åˆ†ç±»ç‰¹å¾ç›¸å…³æ€§")


def save_correlation_report(train_df, output_path=None, target_col='SalePrice', visualize=False):
    """
    ä¿å­˜ç›¸å…³æ€§åˆ†ææŠ¥å‘Š
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        target_col: ç›®æ ‡å˜é‡åˆ—å
        visualize: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    """
    if output_path is None:
        output_path = ROOT_DIR / "correlation_report.txt"
    
    # é‡å®šå‘è¾“å‡ºåˆ°æ–‡ä»¶
    original_stdout = sys.stdout
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            sys.stdout = f
            
            print("=" * 80)
            print("House Prices ç‰¹å¾ç›¸å…³æ€§åˆ†ææŠ¥å‘Š")
            print("=" * 80)
            print(f"\nç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}")
            print(f"æ•°æ®è§„æ¨¡: {train_df.shape[0]} è¡Œ Ã— {train_df.shape[1]} åˆ—")
            
            # è¿è¡Œæ‰€æœ‰åˆ†æ
            correlations = analyze_target_correlations(train_df, target_col, top_n=30)
            analyze_feature_correlations(train_df, target_col)
            analyze_missing_correlations(train_df, target_col)
            analyze_feature_importance_for_engineering(train_df, target_col)
            analyze_categorical_correlations(train_df, target_col)
            
        sys.stdout = original_stdout
        print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        if visualize:
            print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            plot_correlation_heatmap(train_df, target_col, top_n=20)
            plot_top_correlations(train_df, target_col, top_n=15)
            plot_scatter_top_features(train_df, target_col, top_n=6)
            print("âœ“ æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
        
    except Exception as e:
        sys.stdout = original_stdout
        print(f"ä¿å­˜æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


def main():
    parser = argparse.ArgumentParser(description='EDA å’Œç›¸å…³æ€§åˆ†æ')
    parser.add_argument('--data', type=str, default=str(DATA_DIR / 'train.csv'),
                       help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--top-n', type=int, default=20,
                       help='æ˜¾ç¤ºå‰ N ä¸ªç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾')
    parser.add_argument('--threshold', type=float, default=0.7,
                       help='ç‰¹å¾é—´ç›¸å…³æ€§é˜ˆå€¼')
    parser.add_argument('--visualize', action='store_true',
                       help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    train_df = load_data()
    
    # è¿è¡Œåˆ†æ
    correlations = analyze_target_correlations(train_df, top_n=args.top_n)
    analyze_feature_correlations(train_df, threshold=args.threshold)
    analyze_missing_correlations(train_df)
    analyze_feature_importance_for_engineering(train_df)
    analyze_categorical_correlations(train_df, top_n=args.top_n)
    
    # ä¿å­˜æŠ¥å‘Šï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.output:
        save_correlation_report(train_df, args.output, visualize=args.visualize)
    elif args.visualize:
        # å³ä½¿ä¸ä¿å­˜æŠ¥å‘Šï¼Œä¹Ÿç”Ÿæˆå¯è§†åŒ–
        print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        plot_correlation_heatmap(train_df, top_n=args.top_n)
        plot_top_correlations(train_df, top_n=args.top_n)
        plot_scatter_top_features(train_df, top_n=min(6, args.top_n))
        print("âœ“ æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
    
    print("\n" + "=" * 80)
    print("åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print("\nä½¿ç”¨å»ºè®®ï¼š")
    print("1. é‡ç‚¹å…³æ³¨ç›¸å…³ç³»æ•° > 0.5 çš„ç‰¹å¾")
    print("2. å¯¹äºé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹ï¼Œè€ƒè™‘ç‰¹å¾å·¥ç¨‹ï¼ˆç»„åˆæˆ–åˆ é™¤ï¼‰")
    print("3. æ ¹æ®ç›¸å…³æ€§åˆ†æç»“æœæŒ‡å¯¼ç‰¹å¾å·¥ç¨‹æ–¹å‘")
    print("4. ä½¿ç”¨ --visualize å‚æ•°ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print("=" * 80)


if __name__ == '__main__':
    main()

